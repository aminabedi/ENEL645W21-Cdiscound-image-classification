{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a90a8sE_abWS"
   },
   "source": [
    "# Project - Cdiscount Image Classification\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nQSDw9jO3VP9"
   },
   "source": [
    "# Data ingestion\n",
    "The primary training set is a 57GB bson file, having ~15 Million images (180x180 images in Base64 format) of ~7.06 Million products. We have imported the dataset into a MongoDB instance on a VPS, so we were able to query among the records.\n",
    "We have chosen 100 categories, which overally consist of ~246K images of ~110K products.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0NunsFV4jxHc"
   },
   "source": [
    "## Dataset preparation\n",
    "\n",
    "First we need to ensure that the \"gdown\" library is installed and accessible in the environment and download the train_medium data from Google Drive,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sVPqZO_a0mc5",
    "outputId": "23317fca-244e-499b-9ff9-be89855d8198"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/bin/sh: 1: pip: not found\r\n"
     ]
    }
   ],
   "source": [
    "! pip install gdown && gdown --id 1F6Xf4yiYxeFEN6qhrL3YBNs0Vhx0bXJ1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Note for the team\n",
    "Since the original dataset is pretty large, I've created a subset file containing ~250K photos in 100 categories, but it is still so large that it may not fit into the memory, so I've used the below parameters to load a fitable subset accordingly, please read the comments of each variable careflully, and do not change the loading code please, just set the values of the parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import base64\n",
    "from PIL import Image\n",
    "import base64\n",
    "import io\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the cell below if you have a gpu that you want to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "physical_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "if len(physical_devices):\n",
    "    tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "print(physical_devices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l9gg39dI6f-E"
   },
   "source": [
    "## 1. Load your data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONVERT_TO_NP_ARRAY= False   # Wether convert the Base64 string to (180,180,3) arrays or keep the Base64 string.\n",
    "\n",
    "REPLACE_BASE64_SPECIAL_CHARS = True # If you have the base64 decoding layer in your model, \n",
    "                                    # you need to set this to True to replace the two special characters in Base64 that is incompatible with tf image reader\n",
    "\n",
    "LOADING_MODE = \"all\" \n",
    "                             # num_records: Loads the first NUM_RECORDS in the dataset and calculates NUM_CATEGORIES dynamically\n",
    "                             # num_categories: Loads first NUM_CATEGORIES and calculates NUM_RECORDS dynamically\n",
    "                             # all: Loads all the 250K images, ignores all parameters below\n",
    "                            \n",
    "    \n",
    "NUM_RECORDS = 3000           # Only used when the mode is set to num_records\n",
    "NUM_CATEGORIES = 10           # Only used when the mode is set to num_category\n",
    "MAX_RECORDS_PER_CATEGORY = 0 # if not zero, will ensure that there is no more per category in the dataframe, won't work when mode is set to all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_array_from_base64(img, shape=(180,180,3)):\n",
    "    print(img)\n",
    "    return np.array(\n",
    "        Image.open(\n",
    "            io.BytesIO(\n",
    "                base64.b64decode(\n",
    "                    img.replace('_', '/').replace('-', '+') if REPLACE_BASE64_SPECIAL_CHARS else img\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "    ).reshape(shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num records: 3000\n",
      "Num categories: 99\n",
      "Training df shape: (246261, 4)\n",
      "Mem used by images: 1613 MB\n"
     ]
    }
   ],
   "source": [
    "FILE_NAME= \"train_shuffled_100cat.csv\"\n",
    "header = 3\n",
    "\n",
    "df = pd.read_csv(FILE_NAME, header=3, nrows=0)\n",
    "\n",
    "if LOADING_MODE == \"all\":\n",
    "    df = pd.read_csv(FILE_NAME, header=header)\n",
    "\n",
    "if LOADING_MODE == \"num_records\":\n",
    "    reader = pd.read_csv(FILE_NAME, header=header, chunksize=min(NUM_RECORDS, 100))\n",
    "    for chunk in reader:\n",
    "        df = df.append(chunk, ignore_index=True)\n",
    "        if MAX_RECORDS_PER_CATEGORY:\n",
    "            for cat in df[\"category_id\"].unique():\n",
    "                if len(df[df[\"category_id\"] == cat]) > MAX_RECORDS_PER_CATEGORY:\n",
    "                    removed_rows = df[df[\"category_id\"] == cat][MAX_RECORDS_PER_CATEGORY:]\n",
    "                    df = df.drop(removed_rows.index)\n",
    "        if df.shape[0]>=NUM_RECORDS:\n",
    "            df = df.head(NUM_RECORDS)\n",
    "            break\n",
    "    \n",
    "elif LOADING_MODE == \"num_categories\": \n",
    "    reader = pd.read_csv(FILE_NAME, header=header, chunksize=100)\n",
    "    for chunk in reader:\n",
    "        df = df.append(chunk, ignore_index=True)\n",
    "        if df[\"category_id\"].nunique() > NUM_CATEGORIES:\n",
    "            break\n",
    "    if MAX_RECORDS_PER_CATEGORY:\n",
    "        for cat in df[\"category_id\"].unique():\n",
    "            if len(df[df[\"category_id\"] == cat]) > MAX_RECORDS_PER_CATEGORY:\n",
    "                removed_rows = df[df[\"category_id\"] == cat][MAX_RECORDS_PER_CATEGORY:]\n",
    "                df = df.drop(removed_rows.index)\n",
    "\n",
    "    cat_removed = df[\"category_id\"].unique()[NUM_CATEGORIES:]\n",
    "    df = df.loc[~df['category_id'].isin(cat_removed)]\n",
    "    NUM_RECORDS= df.shape[0]\n",
    "\n",
    "if CONVERT_TO_NP_ARRAY:        \n",
    "    df[\"image\"] = df[\"image\"].apply(\n",
    "                    lambda x: get_array_from_base64(x)\n",
    "                )\n",
    "if REPLACE_BASE64_SPECIAL_CHARS:\n",
    "    df[\"image\"] = df[\"image\"].apply(\n",
    "                    lambda x: x.replace('/', '_').replace('+', '-')\n",
    "                )\n",
    "    \n",
    "NUM_CATEGORIES = df['category_id'].nunique()\n",
    "\n",
    "categories = df['category_id'].unique()\n",
    "categories.sort()\n",
    "category_id_map = {k: v for v, k in enumerate(categories)}\n",
    "df[\"class\"] = df[\"category_id\"].apply(lambda x: category_id_map[x])\n",
    "\n",
    "print(\"Num records:\", NUM_RECORDS)\n",
    "print(\"Num categories:\", NUM_CATEGORIES)\n",
    "print(\"Training df shape:\", df.shape)\n",
    "print(\"Mem used by images:\", int(sum(df[\"image\"].apply(lambda x: x.nbytes if type(x)!=str else len(x))/10 ** 6)), \"MB\")\n",
    "# print(len(df.at[0, \"image\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.sample(frac=1)\n",
    "df.reset_index(drop=True, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "W94KecGabAoB",
    "outputId": "6859da90-eede-43a0-8b35-5ff33cb2a1a0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(246261,) (246261,) [44 45 60 36 35 60 60 27 18 44]\n"
     ]
    }
   ],
   "source": [
    "X_dev = np.stack(df[\"image\"]) if CONVERT_TO_NP_ARRAY else np.array(df[\"image\"])\n",
    "Y_dev = np.array(df[\"class\"])\n",
    "print(X_dev.shape,Y_dev.shape, Y_dev[-10:])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R0KQWQTcbjUx"
   },
   "source": [
    "## 2. Explore your data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gSVj9Xm2_Qsl"
   },
   "source": [
    "Showing 10 samples from dev set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 554
    },
    "id": "xxMSDDkB6f-G",
    "outputId": "e7c03f86-55d5-44c0-e646-05e4e9aa25a3",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 12000x6000 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(12, 6), dpi=1000)\n",
    "indexes = np.arange(len(X_dev))\n",
    "np.random.shuffle(indexes)\n",
    "if CONVERT_TO_NP_ARRAY:\n",
    "    for idx in range(10):\n",
    "      plt.subplot(2, 5, idx + 1)\n",
    "      plt.imshow(X_dev[indexes[idx]])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hExX4dqr_XNZ"
   },
   "source": [
    "#Splitting dev set into train/val set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fv6OSGIO_bfG",
    "outputId": "a5042e1c-e82d-4654-8c8e-f494b0f9517d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train: (184695,)\n",
      "X_val: (49252,)\n",
      "X_test: (12314,)\n"
     ]
    }
   ],
   "source": [
    "num_train = int(len(X_dev) * .75) #= splitting point of train/val set\n",
    "num_val = int(len(X_dev) * .2)\n",
    "num_test = len(X_dev) - num_train - num_val\n",
    "\n",
    "X_train = X_dev[indexes[:num_train]]\n",
    "Y_train = Y_dev[indexes[:num_train]]\n",
    "\n",
    "X_val = X_dev[indexes[num_train:-num_test]]\n",
    "Y_val = Y_dev[indexes[num_train:-num_test]]\n",
    "\n",
    "X_test = X_dev[indexes[-num_test:]]\n",
    "Y_test = Y_dev[indexes[-num_test:]]\n",
    "\n",
    "print(\"X_train:\", X_train.shape)\n",
    "print(\"X_val:\", X_val.shape)\n",
    "print(\"X_test:\", X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5sswDlu56f-H"
   },
   "source": [
    "## 3. Represent your labels using one hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XbQ9b9TA6f-I",
    "outputId": "77d2cb47-d639-4d8d-eaf3-93ae9163177b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Y_train [60  9 71]\n",
      "Y_train_oh: [[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0.]]\n",
      "TRAIN: (184695,) (184695,) (184695, 99)\n",
      "VAL: (49252,) (49252,) (49252, 99)\n",
      "TEST: (12314,) (12314,) (12314, 99)\n"
     ]
    }
   ],
   "source": [
    "Y_train_oh = tf.keras.utils.to_categorical(Y_train)\n",
    "Y_val_oh = tf.keras.utils.to_categorical(Y_val)\n",
    "Y_test_oh = tf.keras.utils.to_categorical(Y_test)\n",
    "\n",
    "\n",
    "print(\"Y_train\",  Y_train[:3])\n",
    "print(\"Y_train_oh:\",  Y_train_oh[:3])\n",
    "\n",
    "\n",
    "print(\"TRAIN:\", X_train.shape, Y_train.shape, Y_train_oh.shape)\n",
    "print(\"VAL:\", X_val.shape, Y_val.shape, Y_val_oh.shape)\n",
    "print(\"TEST:\", X_test.shape, Y_test.shape, Y_test_oh.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LfQzijUy6f-J"
   },
   "source": [
    "## 4. Data scaling and Data augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess_and_decode:\n",
    "It is a function which applies to each image input to the model, it first decodes the JPEG base64 encoded image to a tensor, then it scales it based on the sample's min, max, mean and std.\n",
    "Since we are normalizing our data per sample, our normalization is row-wise and a column-wise normalization is not yet an option. We can do this by preprocessing our dataset by batching it if that seemed necessary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_and_decode(img_str, new_shape=(180,180), scaling_mode = 2 ): #scaling_mode= 0: disabled, 1: min-max normalization, 2: standardization\n",
    "    img = tf.io.decode_base64(img_str)\n",
    "    img = tf.image.decode_jpeg(img, channels=3)\n",
    "    img = tf.image.resize(img, new_shape, method=tf.image.ResizeMethod.BILINEAR)\n",
    "   \n",
    "    if scaling_mode == 1: \n",
    "      img_min = tf.math.reduce_min(img, axis=None, keepdims=False, name=None)\n",
    "      img_max = tf.math.reduce_max(img, axis=None, keepdims=False, name=None)\n",
    "      img = ( img - img_min ) / (img_max - img_min)\n",
    "      \n",
    "    elif scaling_mode == 2:\n",
    "      img_mean = tf.math.reduce_mean(img, axis=None, keepdims=False, name=None)\n",
    "      img_std = tf.math.reduce_std(img, axis=None, keepdims=False, name=None)\n",
    "      img = ( img - img_mean ) / img_std\n",
    "      \n",
    "    return img\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### About data augmentation\n",
    "Since our dataset consist of 2-4 image per product, and we have over 2K images per category, data augmentation seems unnecessary.\n",
    "I still have not found a way to do data augmentation on Base64 strings(remember I moved to Base64-> tensor decoding inside the model itself to enable us to do batch training on the whole dataset), but if we come to a need for it, there is definitly a way to do that!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4SbUTvdj6f-S"
   },
   "source": [
    "## Convolutional Model\n",
    "\n",
    "## 5. Define your  model, cost function, optimizer, learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "SycQoiF96f-S"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/amin/.local/lib/python3.8/site-packages/tensorflow/python/util/deprecation.py:605: calling map_fn_v2 (from tensorflow.python.ops.map_fn) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use fn_output_signature instead\n",
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 1)]               0         \n",
      "_________________________________________________________________\n",
      "lambda (Lambda)              (None, 180, 180, 3)       0         \n",
      "_________________________________________________________________\n",
      "conv2d (Conv2D)              (None, 180, 180, 32)      896       \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 180, 180, 32)      9248      \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 180, 180, 32)      0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 90, 90, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 90, 90, 64)        18496     \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 90, 90, 64)        36928     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 90, 90, 64)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 45, 45, 64)        0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 129600)            0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 99)                12830499  \n",
      "=================================================================\n",
      "Total params: 12,896,067\n",
      "Trainable params: 12,896,067\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Define your model here. Include accuracy in the metrics list when you compile it\n",
    "# Experiment with different network architectures, learnig rates, dropout, etc.\n",
    "def my_model_cnn(ishape = (180,180,3), k = NUM_CATEGORIES, lr = 1e-3):\n",
    "    \n",
    "    # The first two layers of the model gets Base64 string as an input and outputs the preprocessed tensor (decoded to tensor and normalized)\n",
    "    input64 = tf.keras.layers.Input(shape=(1,), dtype=\"string\")\n",
    "    img_tensor = tf.keras.layers.Lambda(\n",
    "        lambda img: tf.map_fn(lambda im: preprocess_and_decode(im[0]), img, dtype=\"float32\"))(input64)\n",
    "    \n",
    "    l1 = tf.keras.layers.Conv2D(32, (3,3), padding = 'same', activation= 'relu')(img_tensor)\n",
    "    l2 = tf.keras.layers.Conv2D(32, (3,3), padding = 'same', activation= 'relu')(l1)\n",
    "    l2_drop = tf.keras.layers.Dropout(0.25)(l2)\n",
    "    l3 = tf.keras.layers.MaxPool2D((2,2))(l2_drop)\n",
    "    l4 = tf.keras.layers.Conv2D(64, (3,3), padding = 'same', activation='relu')(l3)\n",
    "    l5 = tf.keras.layers.Conv2D(64, (3,3), padding = 'same', activation='relu')(l4)\n",
    "    l5_drop = tf.keras.layers.Dropout(0.25)(l5)\n",
    "    l6 = tf.keras.layers.MaxPool2D((2,2))(l5_drop)\n",
    "    flat = tf.keras.layers.Flatten()(l6)\n",
    "    out = tf.keras.layers.Dense(k, activation= 'softmax')(flat)\n",
    "    model = tf.keras.models.Model(inputs = input64, outputs = out)\n",
    "    model.compile(optimizer = tf.keras.optimizers.Adam(learning_rate=lr), loss = 'categorical_crossentropy', metrics= [\"accuracy\"])\n",
    "    return model\n",
    "\n",
    "model = my_model_cnn()\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GHNJ0kTU6f-S"
   },
   "source": [
    "## 6. Define your callbacks (save your model, patience, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "WUMIu1AU6f-T"
   },
   "outputs": [],
   "source": [
    "model_name_cnn = \"cdiscount_CCN_small.h5\"\n",
    "\n",
    "# define your callbacks\n",
    "# remember that you need to save the weights of your best model!\n",
    "early_stop = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience = 20)\n",
    "\n",
    "monitor = tf.keras.callbacks.ModelCheckpoint(model_name_cnn, monitor='val_loss',\\\n",
    "                                             verbose=0,save_best_only=True,\\\n",
    "                                             save_weights_only=True,\\\n",
    "                                             mode='min')\n",
    "# Learning rate schedule\n",
    "def scheduler(epoch, lr):\n",
    "    if epoch%10 == 0:\n",
    "        lr = lr/2\n",
    "    return lr\n",
    "\n",
    "lr_schedule = tf.keras.callbacks.LearningRateScheduler(scheduler,verbose = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wTf4Ia126f-T"
   },
   "source": [
    "## 7. Train your model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5L1L1y1W6f-T"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n"
     ]
    }
   ],
   "source": [
    "# train your model - decide for how many epochs\n",
    "model.fit(X_train,Y_train_oh,batch_size = 16, epochs = 150, \\\n",
    "          verbose = 1, callbacks= [early_stop, monitor, lr_schedule],validation_data=(X_val,Y_val_oh))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J1dlyCsu6f-U"
   },
   "source": [
    "## 8. Test your model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2ernzhme6f-U"
   },
   "outputs": [],
   "source": [
    "model.load_weights(model_name_cnn)\n",
    "metrics = model.evaluate(X_test,Y_test_oh)\n",
    "\n",
    "Ypred = model.predict(X_test).argmax(axis = 1)\n",
    "wrong_indexes = np.where(Ypred != Y_test)[0]\n",
    "print(wrong_indexes.size)\n",
    "\n",
    "# Disaplying some samples from the development set\n",
    "sample_indexes = np.random.choice(np.arange(wrong_indexes.shape[0], dtype = int),size = 30, replace = False)\n",
    "plt.figure(figsize = (24,18))\n",
    "for (ii,jj) in enumerate(sample_indexes):\n",
    "    plt.subplot(5,6,ii+1)\n",
    "    plt.imshow(get_array_from_base64(X_test[wrong_indexes[jj]]), cmap = \"gray\")\n",
    "    plt.title(\"Label: %d, predicted: %d\" %(Y_test[wrong_indexes[jj]],Ypred[wrong_indexes[jj]]))\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "assignment02.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
